---
{"dg-publish":true,"permalink":"/topics/mathematics/optimization/","dgHomeLink":true,"dgPassFrontmatter":false}
---


Pour maximiser/minimiser une function analytiquement ou numériquement
- [[___INBOX___/__à trier/Evolutionary algorithms|Evolutionary algorithms]]
	- [[___INBOX___/__à trier/Particle Swarm Optimization (1995)|Particle Swarm Optimization (1995)]]
- [[Topics/IT-Computing/Computer Science/Algorithms/Simulated annealing|Simulated annealing]]
- in **Neural Networks** (only?)
	- [[Topics/Machine Learning/Optimization methods/Gradient descent|Gradient descent]]
	- [[Topics/Machine Learning/Optimization methods/Stochastic Gradient Descent|Stochastic Gradient Descent]]
	- No gradient methods => https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97
-  **L-BFGS**
- conjugate-gradient method
	- to invert the local [[___INBOX___/__à trier/Hessian|Hessian]] ?!
- Newton-like methods
	- Apparu au XVIIe siècle
	- iterative method
- [[___INBOX___/__à trier/Nelder-Mead method (1965)|Nelder-Mead method (1965)]]
- Test functions (artificial landspaces) : to determine robustness, precision, convergence rate
	- Rosenbrock banana function (1960)
	- Himmelblau's function
	- Simionescu's function

### Optimisation quadratique
- la fonction objectif est une **forme quadratique**


---
with [[___INBOX___/__à trier/SciPy|SciPy]] : https://scipy.github.io/devdocs/tutorial/optimize.html
