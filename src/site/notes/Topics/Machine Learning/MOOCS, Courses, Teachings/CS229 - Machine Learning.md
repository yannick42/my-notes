---
{"dg-publish":true,"permalink":"/topics/machine-learning/moocs-courses-teachings/cs-229-machine-learning/"}
---

- **broad introduction** to [[Topics/Machine Learning/Machine Learning|Machine Learning]] and statistical pattern recognition.

> http://cs229.stanford.edu/syllabus.html
> https://see.stanford.edu/Course/CS229

### Prerequisites
- **==basic probability theory==** : **Stat 116** (sufficient but not necessary)
	- **Probability spaces** as models for phenomena with statistical regularity. **Discrete spaces** (binomial, hypergeometric, Poisson). **Continuous spaces** (normal, exponential) and densities. [[___INBOX___/__à trier/Random variables|Random variables]], expectation, independence, [[___INBOX___/__à trier/Probabilité conditionnelle|conditional probability]]. Introduction to **the laws of large numbers** and **[[___INBOX___/__à trier/Théorème central limite|central limit theorem]]**
- **basic linear algebra** :
	- Math 51
	- Math 103
	- Math 113 or CS 205 (would be much more than necessary)

--> [Youtube Playlist (20 videos)](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

### Content/Lectures
> 2 or 3 lectures per week => `September 21th to December 3nd` (2 months & 10 days)
- **Lecture 2**
	- [[Topics/Machine Learning/Concepts/Supervised learning|Supervised learning]]
- **Lecture 3**
	- Weighted Least-Squares
	- [[Topics/Machine Learning/Models/Logistic regression|Logistic regression]]
	- [[Topics/Mathematics/Calculus/Newton’s method|Newton’s method]]
- **Lecture 4**
	- Dataset split
	- [[Topics/Mathematics/Statistics and probabilities/Exponential family|Exponential family]]
		- exponential tilting => ?
	- **GLM** (generalized linear models)
- **Lecture 5**
	- Gaussian discriminant analysis - **GDA**
	- [[___INBOX___/__à trier/Naive Bayes|Naive Bayes]]
- **Lecture 6**
	- Naive Bayes & Laplace smoothing
- **Lecture 7**
	- Kernels & [[Topics/Machine Learning/Models/Support Vector Machine (1992)|SVM]]
- **Lecture 8**
	- [[Topics/Machine Learning/Models/Neural Networks|Neural Networks]]
- **Lecture 9**
	- [[Topics/Machine Learning/Concepts/Basics/Backpropagation (1986)|Backpropagation (1986)]]
- **Lecture 10**
	- Bias - Variance
	- Regularization
	- Feature / Model selection
- **Lecture 11**
	- [[Topics/Machine Learning/Models/Decision Trees|Decision Trees]]
	- [[Topics/Machine Learning/Boosting|Boosting]]
- **Lecture 12**
	- [[K-Means]]
	- GMM (non EM)
	- [[___INBOX___/__à trier/Expectation-maximization algorithm|EM algorithm]]
	- [[Factor Analysis]]
		- https://en.wikipedia.org/wiki/Factor_analysis
- **Lecture 13**
	- [[___INBOX___/__à trier/Expectation-maximization algorithm|EM algorithm]]
	- [[___INBOX___/__à trier/GMM|GMM]]
- **Lecture 14**
	- [[Topics/Mathematics/Statistics and probabilities/PCA (1901)|PCA (1901)]]
	- Types of learning
- **Lecture 15**
	- ML Advices
- **Lecture 16**
	- [[Topics/Machine Learning/Concepts/Unsupervised learning|Unsupervised learning]] & [[Topics/Machine Learning/Concepts/Reinforcement Learning|Reinforcement Learning]]
- **Lecture 17**
	- [[___INBOX___/__à trier/Value-based methods (RL)|Value-based methods (RL)]]
		- value iteration
	- policy iteration
- **Lecture 18**
	- Model-based RL, value function approximator
- **Lecture 19&20**
	- Fairness, algorithmic bias, explainability, privacy

---
- Une version [[Topics/Machine Learning/MOOCS, Courses, Teachings/Coursera|Coursera]] existe : https://www.coursera.org/learn/machine-learning#syllabus
	- 4.9 stars / 4.6M students
