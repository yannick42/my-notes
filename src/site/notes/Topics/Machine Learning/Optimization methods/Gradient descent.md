---
{"dg-publish":true,"permalink":"/topics/machine-learning/optimization-methods/gradient-descent/","dgHomeLink":true,"dgPassFrontmatter":false}
---


- aka. *steepest descent*
- First discovered/suggested by [[___INBOX___/__Ã  trier/Augustin Louis Cauchy|Augustin Louis Cauchy]] in **1847**[^1] ?
	- independently by Hadamard in **1907**
	- also studied by [[Haskell Curry|Haskell Curry]] in **1944**
- first-order iterative [[Topics/Mathematics/Optimization|optimization]] algorithm for finding local minimum
- Evolutions :
	- [[Topics/Machine Learning/Optimization methods/Extensions of SGD/Momentum method (1964)|Momentum method (1964)]]
	- [[Topics/Machine Learning/Optimization methods/Extensions of SGD/Nesterov momentum (1983)|Nesterov momentum (1983)]]
	- [[Topics/Machine Learning/Optimization methods/Stochastic Gradient Descent|Stochastic Gradient Descent]]
- Can be used to solve $Ax-b=0$, but rarely used -> conjugate gradient method (most popular alternative)

---
https://en.wikipedia.org/wiki/Gradient_descent #DIIGO 


[^1]: https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf
