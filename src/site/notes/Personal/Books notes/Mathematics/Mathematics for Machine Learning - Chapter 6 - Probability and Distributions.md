---
{"dg-publish":true,"dg-permalink":"Chapter 6 - Probability and Distributions","permalink":"/Chapter 6 - Probability and Distributions/"}
---

â†‘[[Topics/Published notes/MML (home)|Mathematics for Machine Learning (home)]]
<-- [[Personal/Books notes/Mathematics/Mathematics for Machine Learning - Chapter 5 - Vector Calculus|Chapter 5 - Vector Calculus]] - [[Personal/Books notes/Mathematics/Mathematics for Machine Learning - Chapter 7 - Continuous Optimization|Chapter 7 - Continuous Optimization]] --> 

---

## 6.1 - Construction of a probability space

---
## 6.2

---
## 6.3 - Sum rule, Product rule & Bayes' Theorem

---
## 6.4 - Summary statistics & Independence
- numbers that summarize and characterize a distribution
- ex: mean & variance

### 6.4.1 - Means and Covariances
- expected value
	- mean as special case where $g(x)$ is the identity function ?
- median
- mode
- covariance
- variance
- standard deviation
- covariance matrix
- cross-covariance
- correlation

### 6.4.2 - Empirical Means and Covariances

### 6.4.3 - Three expressions for the Variance
- raw-score formula

### 6.4.4 - Sums and Transformations of Random Variables

### 6.4.5 - Statistical independence

### 6.4.6 - Inner products of Random Variables
- Bregman divergences and f-divergences
	- [[Topics/Mathematics/Statistics and probabilities/Kullback-Leibler (KL) divergence|Kullback-Leibler (KL) divergence]] is a special case
	- Amari (2016)
		- one of the founder of the field of `information geometry`

---
## 6.5 - Gaussian distribution
- the most-well studied distribution
- many computationally convenient properties

### 6.5.1 - Marginals & Conditionals of Gaussians are Gaussians

### 6.5.2 - Product of Gaussian Densities

---
## 6.6 - Conjugacy & Exponential Family
-> [[Topics/Mathematics/Statistics and probabilities/Exponential family|Exponential family]]

## 6.7 - Change of variable / Inverse transform

## 6.8 - Further reading
- For self-study
	- Grinstead and Snell (1997)
	- Walpole et al. (2011)
- 